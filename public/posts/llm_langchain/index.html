<!doctype html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Building an LLM Application with Langchain // AgilePhD</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.128.2">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Dr. James Davis" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.562871a81790ee61fbc41ddb89eacf3bc0967ee9bde46cf8764c66fe91f0e6fc.css" />
    

    
  


    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Building an LLM Application with Langchain">
  <meta name="twitter:description" content="Primarily developed in Python, LangChain is a robust framework designed to simplify the development of applications that utilize large language models (LLMs). It enables developers to connect these models with various data sources and external APIs, facilitating the integration of advanced natural language processing capabilities into their applications. LangChain provides tools for constructing LLM-powered workflows, handling memory and state management, and ensuring scalability and efficiency. By offering a streamlined interface for working with LLMs, LangChain empowers developers to create sophisticated AI-driven solutions in areas such as chatbots, data analysis, content generation, and more, without needing extensive expertise in machine learning or AI model development.">

    <meta property="og:url" content="http://localhost:1313/posts/llm_langchain/">
  <meta property="og:site_name" content="AgilePhD">
  <meta property="og:title" content="Building an LLM Application with Langchain">
  <meta property="og:description" content="Primarily developed in Python, LangChain is a robust framework designed to simplify the development of applications that utilize large language models (LLMs). It enables developers to connect these models with various data sources and external APIs, facilitating the integration of advanced natural language processing capabilities into their applications. LangChain provides tools for constructing LLM-powered workflows, handling memory and state management, and ensuring scalability and efficiency. By offering a streamlined interface for working with LLMs, LangChain empowers developers to create sophisticated AI-driven solutions in areas such as chatbots, data analysis, content generation, and more, without needing extensive expertise in machine learning or AI model development.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-07-22T00:00:00+00:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Langchain">


  </head>
  <body>
    <header class="app-header">
      <a href="http://localhost:1313/"><img class="app-header-avatar" src="/tank1.jpg" alt="Dr. James Davis" /></a>
      <span class="app-header-title">AgilePhD</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>Dr. James Davis: data scientist, girl dad, combat veteran, and can&#39;t shutup about Blockchain.</p>
      <div class="app-header-social">
        
          <a href="https://github.com/cavalryjim" target="_blank" rel="noreferrer noopener me">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://www.linkedin.com/in/jdavisphd/" target="_blank" rel="noreferrer noopener me">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>LinkedIn</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Building an LLM Application with Langchain</h1>
      <div class="post-meta">
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jul 22, 2024
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          4 min read
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="http://localhost:1313/tags/python/">Python</a>
              <a class="tag" href="http://localhost:1313/tags/ai/">AI</a>
              <a class="tag" href="http://localhost:1313/tags/langchain/">Langchain</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>Primarily developed in Python, LangChain is a robust framework designed to simplify the development of applications that utilize large language models (LLMs). It enables developers to connect these models with various data sources and external APIs, facilitating the integration of advanced natural language processing capabilities into their applications. LangChain provides tools for constructing LLM-powered workflows, handling memory and state management, and ensuring scalability and efficiency. By offering a streamlined interface for working with LLMs, LangChain empowers developers to create sophisticated AI-driven solutions in areas such as chatbots, data analysis, content generation, and more, without needing extensive expertise in machine learning or AI model development.</p>
<p>Let&rsquo;s dive into building an LLM-based application!</p>
<h3 id="getting-started">Getting Started</h3>
<p>In this example, we will create a foundational LLM-based application that ingests information from a PDF file and enables interactive questioning about its content.</p>
<p>You can find the code for this post on <a href="https://github.com/cavalryjim/langchain_llm_app">GitHub</a>.  Depending on your experience with LangChain, you might need to run a few <code>pip install</code> commands or simply use my <code>requirements.txt</code> file. Feel free to use a PDF file of your choice; I have also included a policy memo, PM-11, from the university where I teach.</p>
<p>The smart play would also be to create a virtual envrionment for this project.  I like pipenv but you are welcome to use something different.</p>
<pre tabindex="0"><code>$ pipenv shell
$ pip install -r requirements.txt
</code></pre><p>Use whatever manner you want to edit &amp; run the code.  For projects like this, I usually boot up a session using Jupyter Notebooks.</p>
<pre tabindex="0"><code>$ jupyter notebook
</code></pre><p>First, we need to import the necessary libraries and load environment variables:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Import necessary libraries</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dotenv <span style="color:#f92672">import</span> load_dotenv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.document_loaders <span style="color:#f92672">import</span> PyPDFLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> CharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.vectorstores <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> RetrievalQA
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>load_dotenv()
</span></span></code></pre></div><p>In above block, we&rsquo;re importing essential modules for handling PDFs, text processing, embeddings, vector storage, and the language model. The <code>load_dotenv</code> function loads environment variables from a <code>.env</code> file, which is useful for securely managing API keys.</p>
<h3 id="extract-contents-from-pdf">Extract Contents from PDF</h3>
<p>Next, we&rsquo;ll load the PDF and extract its contents:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loader <span style="color:#f92672">=</span> PyPDFLoader(<span style="color:#e6db74">&#34;pm-11.pdf&#34;</span>)
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>policy_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> isinstance(doc, dict) <span style="color:#f92672">and</span> <span style="color:#e6db74">&#39;text&#39;</span> <span style="color:#f92672">in</span> doc:
</span></span><span style="display:flex;"><span>        policy_text <span style="color:#f92672">+=</span> doc[<span style="color:#e6db74">&#39;text&#39;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> isinstance(doc, str):
</span></span><span style="display:flex;"><span>        policy_text <span style="color:#f92672">+=</span> doc
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        policy_text <span style="color:#f92672">+=</span> repr(doc)
</span></span></code></pre></div><p>Here, we use <code>PyPDFLoader</code> to load the PDF file. We then iterate through the loaded data, concatenating text from the document into a single string. This step ensures we have all the text content in a manageable format.</p>
<h3 id="preprocess-the-text">Preprocess the Text</h3>
<p>Now, we preprocess the extracted text and prepare it for further processing:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ct_splitter <span style="color:#f92672">=</span> CharacterTextSplitter(separator<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>docs <span style="color:#f92672">=</span> ct_splitter<span style="color:#f92672">.</span>split_text(policy_text)
</span></span></code></pre></div><p>In this section, <code>CharacterTextSplitter</code> is used to split the text into smaller chunks. By specifying a separator, chunk size, and overlap, we ensure that the text is divided into manageable pieces, which helps in handling large documents effectively.</p>
<h3 id="vectorize-and-store-in-db">Vectorize and Store in DB</h3>
<p>Next, we vectorize the text chunks and store them in a database for efficient retrieval:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>api_key <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;OPENAI_API_KEY&#34;</span>)
</span></span><span style="display:flex;"><span>embedding_function <span style="color:#f92672">=</span> OpenAIEmbeddings(openai_api_key<span style="color:#f92672">=</span>api_key)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> Chroma(persist_directory<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;LSU&#39;</span>, embedding_function<span style="color:#f92672">=</span>embedding_function)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>docstorage <span style="color:#f92672">=</span> Chroma<span style="color:#f92672">.</span>from_texts(docs, embedding_function)
</span></span></code></pre></div><p>Here, we retrieve the OpenAI API key from environment variables and initialize the <code>OpenAIEmbeddings</code> function to convert text into embeddings. We then use <code>Chroma</code> to create a vector database, storing the embeddings of our text chunks, making it easier to retrieve relevant information later.</p>
<p>As a note, Chroma creates a SQLite database file inside your project in a subfolder designated by the <code>persist_directory</code> input.  In my case, it would be a subfolder called &ldquo;LSU&rdquo;.</p>
<h3 id="fine-tune-a-language-model">Fine-Tune a Language Model</h3>
<p>We then configure the language model to interact with our vector database:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>llm<span style="color:#f92672">=</span>OpenAI(model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-3.5-turbo-instruct&#34;</span>, openai_api_key<span style="color:#f92672">=</span>api_key)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>qa <span style="color:#f92672">=</span> RetrievalQA<span style="color:#f92672">.</span>from_chain_type(llm<span style="color:#f92672">=</span>llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stuff&#34;</span>, retriever<span style="color:#f92672">=</span>docstorage<span style="color:#f92672">.</span>as_retriever())  
</span></span></code></pre></div><p>In this block, we initialize the OpenAI language model (<code>gpt-3.5-turbo-instruct</code>) and set up a retrieval-based question-answering system (<code>RetrievalQA</code>). The <code>docstorage.as_retriever()</code> method allows the model to fetch relevant information from our vector database.</p>
<h3 id="query-the-llm">Query the LLM</h3>
<p>Finally, we can query the language model and get answers based on the document content:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>question <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Can I get a blanket approval for work outside of LSU?&#34;</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> qa<span style="color:#f92672">.</span>invoke(question)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(response[<span style="color:#e6db74">&#39;result&#39;</span>])
</span></span></code></pre></div><pre tabindex="0"><code>No, blanket approvals for outside employment will not be granted.
</code></pre><p>Here, we pose a question to the language model, which retrieves relevant information from the vector database and provides a response. This demonstrates the application&rsquo;s ability to interactively answer questions based on the ingested PDF content.</p>
<p>By following these steps, you can harness the power of LangChain to build a versatile, LLM-powered application capable of interactive and dynamic querying of document contents.</p>

    </div>
    
    <div class="post-footer">
      
    </div>
  </article>
  <h4>Comments:</h4>
  <div id="cusdis_thread" data-host="https://cusdis.com"
    data-app-id="00693eb6-18f3-4176-b3ba-cf9245891bce"
    data-page-id="0a75ec4faf637f0cc7e061d91630a7ea" data-page-url="blog.http://localhost:1313/posts/llm_langchain/"
    data-page-title="Building an LLM Application with Langchain"></div>
  <script async defer src="https://cusdis.com/js/cusdis.es.js"></script>

    </main>
  </body>
</html>
